<!DOCTYPE html>
<html lang="en">

<head>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-00W722FWP2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-00W722FWP2');
  </script>
  <title>The Big O Notation</title>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="subject" content='here'>
  <meta name="description" content="here" />
  <meta name="keywords" content="data structures, algorithms, c++, cpp, computer science" />
  <meta name="author" content="Don Le, ledongduu@gmail.com" />
  <meta name="format-detection" content="telephone=no"> <!-- Disable number string recognition-->
  <link rel="stylesheet" href="../../assets/css/main.css">
  <style>
    .front-img {
      display: block;
      width: 100%;
      border-radius: 10px;
      height: auto;
      margin: auto;
    }

    .algorithm_box {
      width: 50%;
    }

    @media all and (max-width: 580px) {
      .algorithm_box {
        width: 100%;
      }
    }

    #nlogn>.MathJax {
      font-size: inherit !important;
    }
  </style>
</head>

<body class="nav-open light-mode">
  <div class="mathjax-definition">
    \[
    \newcommand{\lbrac}{\left(}
    \newcommand{\rbrac}{\right)}
    \]
  </div>
  <nav class="navbar open" id="navbar">
    <!-- SIDE BAR -->
    <div id="logo"></div>
    <div class="side-nav-container"></div>
    <!--open attribute - default-->
    <div class="line-3"></div>
    <header class="major">
      <h2>Table of Contents</h2>
    </header> <!-- TABLE OF CONTENTS -->
    <div class="toc">
      <ul>
        <li>Introduction
          <ul>
            <li>What is the Best Way to Compare Algorithms' Efficiency?</li>
            <li>The Big O, Big Omega, Big Theta, and Their Properties
              <ul>
                <li>The Big-O Notation</li>
                <li>Properties of the Big-O Notation</li>
              </ul>
            </li>
            <li>Algorithmic Analysis</li>
          </ul>
        </li>
        <li>The Fundamental Functions of Algorithmic Analysis
          <ul>
            <li>The Constant Function</li>
            <li>The Logarithm Function</li>
            <li>The Linear Function</li>
            <li>The n log n Function</li>
            <li>The Quadratic Function</li>
            <li>The Cubic Function</li>
            <li>The Exponential Function</li>
            <li>The Factorial Function</li>
            <li>Other Irregular Functions</li>
          </ul>
        </li>
        <li><a>When Does Time Complexity Start to Matter?</a></li>
        <li><a>References</a></li>
      </ul>
    </div>
    <div class="highlights-and-attribute"></div>
  </nav>

  <!--Master grid starts here -->
  <div class="content-grid">

    <!-- CONTENTS -->
    <div class="general-wrapper">


      <header>
        <div class="topic">
          Topics: Computer-Science/Data-Structure/Algorithms/C++
        </div>
        <h1 class="title">Time Complexity of an Algorithm</h1>
        <div class="date"></div>
        <figure style="margin: 0;">
          <img class="front-img" src="time-complexity-banner.png" alt="image">
        </figure>
        <div class="Quote">
          <div class="Quote-content">&#8220Science is what we understand well enough to explain to a computer. Art is
            everything else we do.&#8221</div>
          <div class="Author">- Donald Knuth</div>
        </div>
      </header>



      <section>
        <h2>Introduction</h2>

        <section>
          <h3>What is the Best Way to Compare Algorithms' Efficiency?</h3>
          <p>
            Given two algorithms that perform the same task, what is the best way to compare their performance? At first
            glance, one might consider measuring the runtime of the two algorithms. However, this is not a reliable
            metric, as runtime depends heavily on the hardware and can be highly inconsistent due to real-world
            conditions, even on the same machine. To address this problem, scientists have developed a more theoretical
            approach called the time complexity of an algorithm. This measure evaluates how an algorithm's performance
            scales as the size of the input increases.
          </p>
          <p>
            If $n$ is the number of input, then for a given algorithm, there exists a function $f(n)$ that measures the
            worst-case runtime of the algorithm as $n$ varies. The runtime of an algorithm generally follows a specific
            trend, such as constant, logarithmic, or linear. However, as noted, the runtime is significantly influenced
            by hardware and physical conditions. Therefore, these functions are often expressed with additional constant
            factors, such as:
          </p>
          <div class="equation">
            $$f(n) = C_1n + C_0,$$
          </div>
          <p>
            where $C_1$ and $C_0$ are factors that depends on outside conditions. This expression is quite inconvenient,
            as we don't usually know what these constant factors are, so the general rule of thumb to express the
            worst-case runtime of an algorithm is through the big-O notation. For $f(n) = C_1n + C_0$, for example, if
            $f(n) = C_1n + C_0$, then $f(n)$ it can be said that
          </p>
          <div class="equation">
            $$f(n) \in O(n).$$
          </div>
          <p>
            There are many categories of such functions $f(n)$, and the most important functions used in algorithmic
            analysis are the constant function, $f(n)=c$, the logarithm function $f(n) = \log_b(n)$, the linear function
            $f(n) = n$, $f(n) = n\log(n)$, the quadratic function $f(n) = n^2$, other polynomial functions $f(n) = n^d$,
            the exponential functions $f(n) = a^n$, and the factorial function $f(n) = n!$.
          </p>
        </section>

        <section>
          <h3>The Big O, Big Omega, Big Theta, and Their Properties</h3>

          <section>
            <h4>The Big-O Notation</h4>
            <div class="definition" data-definition-name=" (The Big-Oh)">
              Let $f$ and $g$ be real-valued and not necessarily continuous functions. We say that $f$ is the big O of
              $g$, written
              <div class="equation">
                $$f(n) = O(g(n)),\quad n\to\infty,$$
              </div>
              if there's some large enough value of $n_0$ such that
              <div class="equation">
                $$|f(n)| \leq M|g(n)|,\quad\forall n\geq n_0.$$
              </div>
              this is equivalent to saying that
              <div class="equation">
                $$\lim_{n\to\infty}\frac{f(n)}{g(n)} = A,\quad 0 < A< \infty.$$ </div> Alternatively, we can say that
                  $f$ is order of $g$, mathematically written, <div class="equation">
                  $$f(n)\in O(g(n))$$
              </div>
              for the big-O notation.
            </div>

            <p>
              For example, we say that $f(n) = 3n \in O(n)$ because $|3n| \leq M|n|,$ for $n\geq 0$, and $M$ can be any
              number greater than or equal to 3. Similarly, $8n + 5\in O(n)$ because if we choose $M = 9$ and $n_0 = 5$,
              then $8n + 5 \leq 9n$, for $n\geq 5$.
            </p>
          </section>


          <section>
            <h4>Properties of the Big-O Notation</h4>

            <p>
              Given two functions $f(n)$ and $g(n)$ defined on $n\geq 0$.
            </p>
            <ol style="line-height: 1.6;" type="i">
              <li style="margin: var(--in-page-li-spacing) 0;">
                If
                <div class="equation">
                  $$\lim_{n\to\infty}\frac{f(n)}{g(n)} = A,\quad 0 < A < \infty.$$ </div> then we say that </li> <li
                    style="margin: var(--in-page-li-spacing) 0;">

              </li>
            </ol>
          </section>

        </section>

        <section>
          <h3>Algorithmic Analysis</h3>
          <p>
            How can we determine the time complexity of an algorithm? Consider the following example: finding the
            largest element in an array.
          </p>

          <div style="text-align: center; margin: 25px auto;" class="algorithm_box">
            <pre class="console">
    <samp>// Algorithm: find the maximum element of an array of integers
    // Input: an array A of size n
    // Output: maximum element of A
    array_max(int A[], int n){
        int current_max = A[0];
        for (i = 1 ; i < n; i++)
            if (A[i] > current_max)
                current_max = A[i];
        return current_max;
     } </samp></pre>
          </div>

          <p>
            The assignment operator <code class="inline-code">current_max = A[0]</code> takes constant time, $f(n)=c$.
            In the for loop, the operator <code class="inline-code">i = 1</code> also takes constant time, $f(n)=c$. The
            comparison operator, also takes constant time, but the operator repeated $n$ times, so the runtime function
            is $f(n) = c_1n + c_0$.
          </p>
        </section>
      </section>






      <section>
        <h2>The Fundamental Functions of Algorithmic Analysis</h2>

        <section>
          <h3>The constant function</h3>
          <p>
            The constant time complexity function, denoted
          </p>
          <div class="equation">
            $$f(n) = c \in O(1),$$
          </div>
          <p>
            does not depend on the size of the input (unless the size of the input is exceptionally large). Some
            examples of algorithms with this complexity are:
          </p>
          <ul style="line-height: 1.6;">
            <li style="margin: var(--in-page-li-spacing) 0;">Accessing array index: Accessing elements at a specific
              index in an array takes constant time because an array begins at a fixed memory location. The memory
              location of each element can be directly calculated using the element's size and its index, which is a
              constant operation.

            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">
              <p>
                Basic arithmetic and bitwise operations: Computers usually uses 8-bit, 16-bit, 32-bit, and 64-bit for
                integers and floating point numbers. Addition, subtraction, multiplication, or division of two numbers
                are considered constant time complexity because they are limited to a certain amount of bit. However,
                for large number arithmetic with arbitrary precision, the time complexity will no longer be constant.
              </p>
              <p>
                Bitwise operations like AND, OR, and XOR also take constant time because, again, primitive data types
                are limited to a certain size on the computer.
              </p>
            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">Variable assignment: For primitive data types such as
              integers, floating points, characters, and boolean, the assignment operator takes constant time complexity
              because the size of these variables are pre-determined. However, for non-primitive data types such as
              strings, the assignment operator takes linear time, $O(n)$ because strings are arrays of characters.

            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">Returning a value from a function: Returning a pre-defined
              value also takes constant time.

            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">
              Inserting a node, or jump to the next/previous node in a linked list: These operation are constant because
              each node holds the address of the next node (for a singly linked list), or the address of both the
              previous and the next node (for a doubly linked list).
            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">
              Pushing and popping on stack. Insertion and removal from queue.
            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">Hashmap operations: Insert, access, and search operations
              in a hash map take constant time on average (ideally). These operations are $O(1)$ on average because a
              hash map uses a bucket array, and accessing elements in an array is $O(1)$. The hash function computes an
              index for a given key, and ideally, keys are distributed evenly across the buckets. However, if all keys
              end up in the same bucket due to hash collisions (e.g., in separate chaining), these operations degrade to
              $O(n)$ in the worst case, where $n$ is the total number of keys in the hash map.

            </li>
            </ol>
        </section>


        <section>
          <h3>The Logarithm Function</h3>
          <p>
            Some examples of this type of algorithms are:
          </p>
          <ul style="line-height: 1.6;">
            <li style="margin: var(--in-page-li-spacing) 0;">Binary search: binary search involves searching for an
              element in a sorted array. This algorithm is $O(\log n)$ because it utilizes the divide and conquer
              technique, where the size of the array reduces by half every iteration.

            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">Calculate the 64-bit (or less) $n$-th Fibinacci number with
              <a href="https://www.geeksforgeeks.org/fast-doubling-method-to-find-the-nth-fibonacci-number/"
                class="url">fast doubling method</a>.

            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">Balanced Binary Search Trees (e.g., AVL Tree, Red-Black
              Tree): Searching, insertion, and deletion in balanced binary search trees like AVL trees, Red-Black trees,
              or B-trees operate in $O(\log n)$ because the height of a balanced tree is $O(\log n)$.

            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">Binary Heap Operations: Operations like insertion and
              extracting the minimum (or maximum) in a binary heap take $O(\log n)$ time because of heapify operations.
            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">Exponentiation by Squaring: This algorithm utilizes the
              divide and conquer method by checking whether $n$ is even or odd.
              <div style="text-align: center; margin: 25px auto;" class="algorithm_box">
                <pre class="console">
    <samp>Input: a real number x; an integer n
    Output: x^n
    exp_by_squaring(x, n)
        if n < 0 then
            return exp_by_squaring(1 / x, -n);
        else if n = 0 then 
            return 1;
        else if n is even then 
            return exp_by_squaring(x * x, n / 2);
        else if n is odd then 
            return x * exp_by_squaring(x * x, (n - 1) / 2);
    end function </samp></pre>
              </div>
            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">Finding the Greatest Common Divisor (GCD)
            </li>
          </ul>
        </section>



        <section>
          <h3>The Linear Function</h3>


          <ul style="line-height: 1.6;">
            <li style="margin: var(--in-page-li-spacing) 0;">
              Finding the maximum/minimum of an array: We can only find the maximum or the minimum of an array by going
              over all elements of the array.
            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">
              Linear search: If the elements are not sorted, then to find an element in an array, we must perform linear
              search, which has the worst-case complexity of $O(n)$. Similarly, traversing a linked list is also $O(n)$.
            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">
              Deletion and Insertion at random position in a linked list or array. This tasks requires us to traverse
              over each individual elements/nodes, which has the worst-case complexity of $O(n)$.
            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">
              Comparing arrays, linked list, strings.
            </li>
          </ul>
        </section>


        <section>
          <h3>The <span id="nlogn">$n \log n$</span> Function</h3>
          <p>
            These types of algorithms are usually optimized versions of $O(n^2)$ algorithms. Some examples of this type
            of algorithms are:
          </p>
          <ul style="line-height: 1.6;">
            <li style="margin: var(--in-page-li-spacing) 0;">
              Fast Fourier Transform: FFT works by breaking down the computation of the Discrete Fourier Transform
              (DFT), which is $O(n^2)$ nto smaller subproblems. For an input signal of size $n$, the FFT algorithm
              splits it into two halves: one for the even-indexed terms and one for the odd-indexed terms. It then
              recursively computes the DFT of these smaller subproblems: the even and odd indexes terms of the DFT.
            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">
              Merge Sort, Quick Sort (on average), Heap Sort, AVL Tree Sort are all $O(n\;\log n)$ algorithms.
            </li>
          </ul>
        </section>


        <section>
          <h3>The Exponential Function</h3>
          <p>
            Algorithms of this type are generally very bad. Some examples are:
          </p>
          <ul style="line-height: 1.6;">
            <li style="margin: var(--in-page-li-spacing) 0;">
              Subset Sum Problem (Brute Force): Given a set of integers, determine if there is a subset whose sum equals
              a given target value. A brute-force approach would involve checking every possible subset of the set to
              see if its sum equals the target. Time Complexity: $O(2^n)$ because there are
              <div class="equation">
                $$S = \sum_{k=0}^{n}\binom{n}{k} = \binom{n}{1} + \binom{n}{2} + \ldots + \binom{n}{n} = 2^n.$$
              </div>
              subsets of a set of $n$ elements.
            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">
              Naive Recursive Fibonacci: Compute the n-th Fibonacci number, where $F(n) = F(n-1) + F(n-2)$ and the base
              cases are $F(0) = 0$ and $F(1) = 1$. The time complexity of this problem is $O(2^n)$ because the recursive
              calls grow exponentially due to repeated subproblems.
            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">
              Find all sub-arrays of an array. This algorithm actually has an time complexity of $O(n\cdot 2^n)$ because
              the total number of non-empty sub-arrays in an array of size $n$ is $2^n - 1$ and we also need to loop
              over $n$ elements of the array.
            </li>
          </ul>
        </section>

        <section>
          <h3>The Factorial Function</h3>
          <ul style="line-height: 1.6;">
            <li style="margin: var(--in-page-li-spacing) 0;">
              The Traveling Salesman Problem (TSP) - Brute Force Approach: Given a set of cities, find the shortest
              possible route that visits each city exactly once and returns to the origin city. A brute force approach
              checks all possible permutations of the cities to find the optimal solution.
            </li>
            <li style="margin: var(--in-page-li-spacing) 0;">
              Generating Factorials: Given a set of integers $S=\{1, 2, \ldots, n\}$. Since there are up to $n!$
              permutations of elements of this set, generating all possible permutation will requires $O(n!)$.
            </li>
          </ul>
        </section>
      </section>


      <section>
        <h2>When Does Time Complexity Start to Matter?</h2>

        <p>
          With the speed of the modern computers, it is not quite obvious to notice the slight change in runtime between
          different time complexities, especially when running only a small code segment, and especially if you are a
          beginner. To notice the impact of how you design your algorithm, we can actually plot out the runtime as $n$
          increases. Below is a C++ program and a python script to verify the runtime.
        </p>
        <p>
          Consider the following problem: Given an array $A = \{a_1, a_2, \ldots, a_n\}$.
        </p>

        <div class="box" style="margin: 30px 0;">
          <div class="tab-button-container">
            <button class="tab-button tab-effect" onclick="changeTab(event, 'main-cpp', 'pseudo-tab')">main.cpp</button>
            <button class="tab-button"
              onclick="changeTab(event, 'maximum_summation-h', 'pseudo-tab')">maximum_summation.h</button>
            <button class="tab-button"
              onclick="changeTab(event, 'time_measure-py', 'pseudo-tab')">time_measurement.py</button>
          </div>
          <div class="code-container pseudo-tab" id="main-cpp">
            <button class="copy-btn" style="position: absolute">Copy</button>
            <pre style="border-radius: 0px 7px 7px 7px;" class="line-numbers"><code class="language-cpp"><script type="prism-cpp">// program to find a maximum contiguous sub-array.
#include <iostream>
#include <vector>
#include <chrono>
#include <ctime>
#include <cstdlib>
#include <iomanip>
#include <fstream> 
#include <string> 
#include "maximum_summation.h"

using namespace std;
using namespace std::chrono;

int main(){
    vector <int> A;
    int upper_bound = 10000;
    int lower_bound = -10000;

    for(int j = 0; j < 3; j++){
        string fileName = "python/timing_data" + to_string(j + 1)+".txt";
        ofstream file(fileName);

        cout << left << setw(10) << "n" << setw(15) << "time (milliseconds)" << endl;
        cout << string(25, '-') << endl;

        file << "n,time_ms" << endl;
        for(int n = 1; n <= 500; n += 1) {
            srand(time(0) + n);
            A.reserve(n); 

            for (int i = 0; i < n; ++i) {
                int random_int = (rand()%(upper_bound - lower_bound)) + lower_bound;
                A[i] = random_int;
            }

            auto start = high_resolution_clock::now();
            f[j](A);
            auto end = high_resolution_clock::now();

            auto duration_ms = duration_cast<duration<double, milli>>(end - start);

            cout << left << setw(10) << n << setw(20) << fixed << setprecision(6) << duration_ms.count() << endl;
            file << n << "," << fixed << setprecision(3) << duration_ms.count() << endl;

            A.clear();
            
        }
    }

    // Run: g++ main.cpp -o main.exe; ./main.exe
    return 0;
}
</script></code></pre>
          </div>




          <div class="code-container pseudo-tab" id="maximum_summation-h" style="display: none">
            <button class="copy-btn" style="position: absolute">Copy</button>
            <pre style="border-radius: 0px 7px 7px 7px;" class="line-numbers"><code class="language-cpp"><script type="prism-cpp">#pragma once

// Input: an array
// Output: the largest sum found in a sub-array
// there should be sum_{i = 0}^{size} binom(size, i) = 2^n sub-arrays
// and there should be 1 + 2 + ... + n = n(n + 1)/2 consecutive sub-arrays
int maximumSummation1(const std::vector<int>& A) {
    if (A.size() == 0) return 0;

    int maxSoFar = A[0];
    int maxEndingHere = A[0];

    for (int i = 1; i < A.size(); i++) {
        maxEndingHere = std::max(A[i], maxEndingHere + A[i]);   // O(1)
        maxSoFar = std::max(maxSoFar, maxEndingHere);           // O(1)
    }

    return maxSoFar;
}

int maximumSummation2(const std::vector<int>& A){
    if (A.size() == 0) return 0;

    int max = A[0];
    for(int i = 0; i < A.size(); i++){
        int sum = 0;
        for(int j = i; j < A.size(); j++){
            sum += A[j];
            if(sum > max)
                max = sum;
        }
    }
    return max;
}   

int maximumSummation3(const std::vector<int>& A){
    if (A.size() == 0) return 0;

    int max = A[0];
    for(int i = 0; i < A.size(); i++){
        for(int j = i; j < A.size(); j++){
            int sum = 0;
            for(int k = i; k <= j; k++){
                sum += A[k];
            }
            if(sum > max)
                max = sum;
        }
    }
    return max;
}

typedef int (*Function_Vector) (const std::vector<int>& A);
Function_Vector f[] = {maximumSummation1, maximumSummation2, maximumSummation3};
    
</script></code></pre>
          </div>


          <div class="code-container pseudo-tab" id="time_measure-py" style="display: none">
            <button class="copy-btn" style="position: absolute">Copy</button>
            <pre style="border-radius: 0px 7px 7px 7px;" class="line-numbers"><code class="language-python">import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import font_manager as fm

# Load the font for the plot
font_path = 'fonts/computer-modern/cmunrm.ttf'
prop = fm.FontProperties(fname=font_path)
plt.rcParams['font.family'] = prop.get_name()

# Load the data from the files
file_names = ['python/timing_data1.txt', 'python/timing_data2.txt', 'python/timing_data3.txt']
data_frames = [pd.read_csv(file) for file in file_names]

# Prepare a list of colors for each dataset
colors = ['red', 'green', 'blue']

# Plot the cleaned data for each file
plt.figure(figsize=(6, 6))
plt.grid(True, linestyle='--')
font_size = 14

for i, data in enumerate(data_frames):
    # Calculate IQR for the current dataset
    Q1 = data[['n', 'time_ms']].quantile(0.25)
    Q3 = data[['n', 'time_ms']].quantile(0.75)
    IQR = Q3 - Q1
    
    # Define outlier thresholds
    outlier_factor = 1.0
    lower_bound = (Q1 - outlier_factor * IQR).to_dict()
    upper_bound = (Q3 + outlier_factor * IQR).to_dict()
    
    # Filter out outliers
    data_clean = data[~((data[['n', 'time_ms']] < lower_bound) | (data[['n', 'time_ms']] > upper_bound)).any(axis=1)]
    
    # Sort data_clean by 'n' to connect the points in order
    data_clean = data_clean.sort_values('n')
    
    # Fit a linear model
    coefficients = np.polyfit(data_clean['n'], data_clean['time_ms'], i + 1)
    linear_fit = np.poly1d(coefficients)
    
    # Plot data points and linear fit
    plt.plot(data_clean['n'], data_clean['time_ms'], marker='o', linestyle='None', 
             color=colors[i], markersize=3, label=f'maximumSummation{i+1}')
    # plt.plot(data_clean['n'], linear_fit(data_clean['n']), color=colors[i], linestyle='-', label=f'maximumSummation{i+1}')

# Set plot labels and title
plt.xlabel('n (Number of Elements)', fontsize=font_size)
plt.ylabel('Time (milliseconds)', fontsize=font_size)
plt.title('n vs Time (milliseconds)', fontsize=font_size)
plt.legend()

plt.show()
</code></pre>
          </div>


        </div>
      </section>

      <section>
        <h2>More Articles</h2>
        <div id="rec-article-container"></div>
      </section>


    </div>
    <div class="footer"></div>
  </div>

  <script src="../../assets/js/scripts.js"></script>
  <script src="../../assets/js/blogpage-setting.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-c.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-cpp.min.js"></script>
  <!-- use prism-c.min.js or prism-clike.min.js-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/line-numbers/prism-line-numbers.min.js">
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" async></script>
</body>

</html>